//
// Copyright (c) Microsoft. All rights reserved.
// Licensed under the MIT license. See LICENSE.md file in the project root for full license information.
//
// ReaderShim.h: Currently we are preserving the old interface in SGD. So this shim exposes the old interface and calls into the 
// reader implemented with the new interfaces (reader/packer/transforms/serializers)
//

#pragma once

#include <unordered_map>
#include <string>
#include <future>
#include "DataReader.h"
#include "Reader.h"

namespace CNTK
{
    class CompositeMinibatchSource;
}

namespace Microsoft { namespace MSR { namespace CNTK {

typedef ReaderPtr (*ReaderFactory)(const ConfigParameters& parameters);

class WorkQueue {
public:
    /// <summary> Constructors a new work queue object. </summary>
    /// <param name="numWorkers"> (Optional) number of workers, less than 0 to 
    ///     auto-detect (may fail on esoteric platforms). </param>
    explicit WorkQueue(int numWorkers = 2) {
        while (numWorkers--) {
            m_workers.emplace_back(std::thread(&WorkQueue::doWork, this));
        }
    }

    /// <summary> Will abort all pending jobs and run any in-progress jobs to 
    ///     completion upon destruction. </summary>
    ~WorkQueue() {
        abort();
    }

    /// <summary> Stops work queue and finishes jobs currently being executed.
    ///     Queued jobs that have not begun execution will have their promises 
    ///     broken. </summary>
    void abort() {
        m_exit = true;
        m_finish_work = false;
        m_signal.notify_all();
        joinAll();

        {
            std::lock_guard<std::mutex> lg(m_mutex);
            m_work.clear();
        }
    }

    /// <summary> Stops new work from being submitted to this work queue.</summary>
    void stop() {
        m_exit = true;
        m_finish_work = true;
        m_signal.notify_all();
    }

    /// <summary> Wait for completion of all submitted work. No more work will 
    ///     be allowed to be submitted. </summary>
    void waitForCompletion() {
        stop();
        joinAll();
        assert(m_work.empty());
    }

    /// <summary> Executes the given function asynchronously. </summary>
    /// <exception cref="std::runtime_error"> Thrown if attempting to submit a job
    ///     to a work queue that is terminating. </exception>
    /// <param name="function"> [in] The function to execute. </param>
    /// <returns> A std::future<RETVAL> for the result that will be generated by 
    ///     the function argument. Exceptions from the function will be 
    ///     thrown by get() on the future.</returns>
    template<typename RETVAL>
    std::future<RETVAL> submit(std::function<RETVAL()>&& function) {
        if (m_exit) {
            throw std::runtime_error("Caught work submission to work queue that is desisting.");
        }

        // Workaround for lack of lambda move capture
        typedef std::pair<std::promise<RETVAL>, std::function<RETVAL()>> pair_t;
        std::shared_ptr<pair_t> data = std::make_shared<pair_t>(std::promise<RETVAL>(), std::move(function));

        std::future<RETVAL> future = data->first.get_future();

        {
            std::lock_guard<std::mutex> lg(m_mutex);
            m_work.emplace_back([data]() {
                try {
                    data->first.set_value(data->second());
                }
                catch (...) {
                    data->first.set_exception(std::current_exception());
                }
            });
        }
        m_signal.notify_one();
        return std::move(future);
    }

private:
    std::deque<std::function<void()>> m_work;
    std::mutex m_mutex;
    std::condition_variable m_signal;
    std::atomic<bool> m_exit{ false };
    std::atomic<bool> m_finish_work{ true };
    std::vector<std::thread> m_workers;

    void doWork() {
        std::unique_lock<std::mutex> ul(m_mutex);
        while (!m_exit || (m_finish_work && !m_work.empty())) {
            if (!m_work.empty()) {
                std::function<void()> work(std::move(m_work.front()));
                m_work.pop_front();
                ul.unlock();
                work();
                ul.lock();
            }
            else {
                m_signal.wait(ul);
            }
        }
    }

    void joinAll() {
        std::vector<std::thread> workers;
        {
            std::lock_guard<std::mutex> lg(m_mutex);
            workers = std::move(m_workers);
        }
        for (auto& thread : workers) {
            thread.join();
        }
    }

    void operator=(const WorkQueue&) = delete;
    WorkQueue(const WorkQueue&) = delete;
};

template <class ElemType>
class ReaderShim : public IDataReader
{
    friend class ::CNTK::CompositeMinibatchSource;
private:
    ReaderShim();

public:
    explicit ReaderShim(ReaderFactory factory);
    explicit ReaderShim(ReaderPtr reader);

    virtual ~ReaderShim() { }

    virtual void Init(const ScriptableObjects::IConfigRecord& /*config*/) override
    {
        assert(false);
    }
    virtual void Init(const ConfigParameters& config) override;

    virtual void Destroy() override
    {
        // Make sure there are no outstanding reads.
        // Future destructor does not wait as of 2013 so probably it is not in VS2013:
        // More info can be found here http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3679.html.
        if (m_prefetchTask.valid())
        {
            // If there are some, give them time to finish.
            m_prefetchTask.wait_for(std::chrono::seconds(5));
            // TODO: if the prefetch is still valid, print a warning here!
        }

        delete this;
    }

    virtual void StartMinibatchLoop(size_t mbSize, size_t epoch, const std::unordered_set<InputStreamDescription>& inputs, size_t requestedEpochSamples = requestDataSize) override;
    virtual void StartDistributedMinibatchLoop(size_t requestedMBSize, size_t epoch, size_t subsetNum, size_t numSubsets, const std::unordered_set<InputStreamDescription>& inputs, size_t requestedEpochSamples) override;

    void StartEpoch(const EpochConfiguration& epoch, const std::unordered_set<InputStreamDescription>& inputs);

    virtual void StartMinibatchLoop(size_t, size_t, size_t) override
    {
        LogicError("Legacy StartMinibatchLoop is not implemented.");
    }

    virtual void StartDistributedMinibatchLoop(size_t, size_t, size_t, size_t, size_t) override
    {
        LogicError("Legacy StartDistributedMinibatchLoop is not implemented.");
    }

    virtual bool SupportsDistributedMBRead() const override
    {
        return true;
    }

    virtual bool IsLegacyReader() const override
    {
        return false;
    }

    virtual bool GetMinibatch(StreamMinibatchInputs& matrices) override;

    virtual bool DataEnd() override;

    void CopyMBLayoutTo(MBLayoutPtr) override;

    virtual size_t GetNumParallelSequencesForFixingBPTTMode() override;

    virtual size_t GetCurrentSamplePosition() override;

    void SetCurrentSamplePosition(size_t currentSamplePosition);

    void SetConfiguration(const ReaderConfiguration& config, const std::map<std::wstring, int>& inputDescriptions);

    bool IsEndOfEpoch() const
    {
        return m_endOfEpoch;
    }

    bool IsEndOfSweep() const
    {
        return m_endOfSweep;
    }

private:
    struct PrefetchResult
    {
        bool m_isEndOfSweep;
        bool m_isEndOfEpoch;
        bool m_isDataAvailable;
    };

    PrefetchResult PrefetchMinibatch(size_t currentDataTransferIndex);

    std::future<PrefetchResult> m_prefetchTask;
    ReaderPtr m_reader;
    ReaderFactory m_factory;
    bool m_endOfEpoch;
    bool m_endOfSweep;

    size_t m_numParallelSequences;

    std::unordered_map<std::wstring, size_t> m_nameToStreamId;
    std::vector<StreamDescriptionPtr> m_streams;
    launch m_launchType;

    WorkQueue m_workQueue;

    // Data structure required for prefetch.
    struct StreamPrefetchBuffer
    {
        std::shared_ptr<Matrix<ElemType>> m_matrix;
        MBLayoutPtr m_mbLayout;
    };

    // Intermediate buffer where the prefetch thread puts its data to.
    // When the main thread enters GetMinibatch it swaps the matrices from this buffer,
    // triggers the next prefetch and waits if memCpy is still in progress.
    std::unordered_map<std::wstring, StreamPrefetchBuffer> m_prefetchBuffers;

    // Alternating data transfer operations. In the current version these are only two - 
    // currently waiting on the main thread and the one that can be started by the prefetch thread 
    // in the meantime.
    std::vector<DataTransfererPtr> m_dataTransferers;

    // Current data transfer. Flips 0 and 1.
    // Can be changed only from the main thread with no ongoing prefetch.
    size_t m_currentDataTransferIndex; 

    // Device id.
    int m_deviceId;

    // Current sample position of the reader on the global timeline.
    // We have to remember the value locally before starting prefetch.
    // The value is updated only from the main thread (in StartEpoch/GetMinibatch)
    size_t m_currentSamplePosition;

    static void FillMatrixFromStream(
        StorageType type,
        Matrix<ElemType>* matrix,
        size_t numRows,
        const StreamMinibatchPtr& stream,
        DataTransferer* transferer);
};

}}}
